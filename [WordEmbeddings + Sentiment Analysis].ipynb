{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "from gensim.utils import tokenize, deaccent, simple_preprocess\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Νευρωνικά Δίκτυα και word embeddings\n",
    "\n",
    "### Περιεχόμενα\n",
    "\n",
    "- Βασικές έννοιες\n",
    "- Απόσταση και ομοιότητα\n",
    "- Word embeddings \n",
    "- Ταξινόμηση κειμένων \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Βασικές έννοιες"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Κωδικοποίηση κειμένων"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### dataset: imbd movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir ./data\n",
    "#!rm ./data/*\n",
    "#!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -O ./data/dataset.tar.gz\n",
    "#!tar xfz  ./data/dataset.tar.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_train = glob.glob(\"./data/aclImdb/train/pos/*.txt\")\n",
    "negative_train = glob.glob(\"./data/aclImdb/train/neg/*.txt\")\n",
    "#negative_train[0:5], positive_train[0:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500, 12500)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_test = glob.glob(\"./data/aclImdb/test/pos/*.txt\")\n",
    "negative_test = glob.glob(\"./data/aclImdb/test/neg/*.txt\")\n",
    "len(negative_test), len(positive_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To all the reviewers on this page, I would have to say this movie is worth seeing. So It was made in 1972, so what. The fashion in the movie was exactly the same fashion of its time. People who didn't study culture of the decades would think that this movie is a cheese ball. Compared to the modern series, `Left Behind,' (Which is made for our time right now) it does look cheezy. However, the only \n",
      "==============\n",
      "The movie confuses religious ethics and ideals so much that it fails to create coherent argument against the death penalty on any level. By presenting the lawful execution of a convicted murder as the catalyst for the apocalyptic end of mankind the movie elevates a parent killer to the status of martyr for Christ. Somehow, according to the plot, god is outraged that society has chosen to rid it's \n"
     ]
    }
   ],
   "source": [
    "sample_pos = open(positive_train[23]).read()\n",
    "print( sample_pos[0:400] )\n",
    "\n",
    "print(\"==============\")\n",
    "sample_neg = open(negative_train[32]).read()\n",
    "print(sample_neg[0:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Αναπαράσταση \n",
    "\n",
    "Θέλουμε να αναπαραστήσουμε τις λέξεις σε μορφή διανύσματος έτσι ώστε να μπορέσουμε να τις δώσουμε ως είσοδο σε ένα νευρωνικό δίκτυο.\n",
    "\n",
    "Ένας τρόπος να γίνει αυτό είναι η κωδικοποίηση \"1-hot-encoding\"\n",
    "Στην κωδικοποίηση αυτή, βρίσκουμε όλες τις λέξεις των κειμένων μας και χτίζουμε ένα λεξικό ($V$), μια λίστα για παράδειγμα με όλες τις λέξεις που εμφάνίζονται στα κείμενα. Κάθε λέξη στη λίστα αυτή (λεξικό) εμφανίζεται μόνο μία φορά. \n",
    "\n",
    "\n",
    "Έστω ότι το λεξικό μας έχει $|V|$ λέξεις. Για κάθε λέξη δημιουργούμε ένα διάνυσμα $|V|$ θέσεων, στο οποίο η θέση στην οποία αντιστοιχεί στη θέση της λέξης στη λίστα η τιμή είναι 1, μηδέν διαφορετικά. \n",
    "\n",
    "\n",
    "Αν δηλαδή το λεξικό μας έχει τις λέξεις [\"και\", \"το\", \"να\"], το διάνυσμα που αντιστοιχεί στη λέξη \"και\" είναι το [1,0,0], στη λέξη \"το\", το διάνυσμα [0,1,0] και στη λέξη \"να\" το διάνυσμα [0,0,1]\n",
    "\n",
    "\n",
    "Στην πράξη, επειδή ο αριθμός των λέξεων που εμφανίζονται στα κείμενα μπορεί να γίνει πολύ μεγάλος, συνήθως αφαιρούμε λέξεις με λιγότερες απο k εμφανίσεις.\n",
    "\n",
    "Επίσης, για λόγους απλότητας και για να κρατήσουμε τον αριθμό λέξεων μικρό, κάνουμε όλα τα κείμενα lower case και παραλείπουμε σημεία στίξης"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab( files  ):\n",
    "    \n",
    "    counter = Counter()\n",
    "    for f in files:\n",
    "        text = open(f).read().lower()\n",
    "        tokens = list(tokenize(deaccent(text)))\n",
    "        counter.update(tokens)\n",
    "    return counter\n",
    "\n",
    "\n",
    "input_files = positive_train + negative_train\n",
    "vocab_freqs = build_vocab( input_files ).most_common(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### χτίσιμο λεξικού"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [word for word,_ in vocab_freqs]\n",
    "word2idx = {word:i for i,word in enumerate(vocab)}\n",
    "idx2word = {i:word for i,word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'and', 'a', 'of', 'to', 'is', 'br', 'it', 'in', 'i']"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vector( word2idx, word):\n",
    "    vec = np.zeros(len(word2idx),dtype=np.int32)\n",
    "    vec[word2idx[word]] = 1\n",
    "    return vec\n",
    "\n",
    "v1 = get_vector(word2idx, \"the\")\n",
    "v2 = get_vector(word2idx, \"of\")\n",
    "\n",
    "v2[0:10]\n",
    "vocab[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ομοιότητα διανυσμάτων\n",
    "\n",
    "θα θέλαμε να μετρήσουμε πόσο \"παρόμοια\" είναι 2 διανύσματα και ιδανικά, παρόμοιες σημασιολογικά λέξεις να έχουν μικρή απόσταση σε σχέση με λέξεις που δεν έχουν σημασιολογική συνάφεια\n",
    "\n",
    "\n",
    "Από τη γραμμική άλγεβρα και την αναλυτική γεωμετρία, ένας τρόπος να μετρηθεί η ομοιότητα 2 διανυσμάτων $u, v$ είναι το εσωτερικό τους γινόμενο\n",
    "\n",
    "$v \\cdot u$. Γενικότερα όμως, επειδή δε μας ενδιαφέρει το \"μήκος\" των διανυσμάτων αλλά το να δείχνουν προς την ίδια κατεύθυνση, στην πράξη χρησιμοποιούμε την ομοιότητα συνημιτόνου, η οποία ορίζεται ώς\n",
    "\n",
    "$$ sim_{cos} =  \\frac{ v \\cdot u }{ ||v|| \\cdot || u ||} \n",
    "\\overset{\\Delta}{=}  \n",
    "\\frac{ \\sum_{i}^{|V|}{u_i * v_i}} { \\sqrt{ \\sum_i^{|V|}{u_i^2} } \\sqrt{ \\sum_i^{|V|}{v_i^2} } }  $$  \n",
    "\n",
    "\n",
    "\n",
    "Στην περίπτωση όμως του 1-hot encoding, όλα τα πιθανά ζεύγη διανυσμάτων u, v με $ u \\neq v$, έχουν απόσταση 1. \n",
    "\n",
    "\n",
    "Η απόσταση ορίζεται ως $d(u,v) = 1-sim_{cos}(u,v)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Neural language models και word2vec\n",
    "\n",
    "\n",
    "Ένα (στατιστικό) γλωσσικό μοντέλο είναι μια κατανομή ακολουθιών λέξεων. Μας ενδιαφέρει να μοντελοποιήσουμε την πιθανότητα \n",
    "\n",
    "$p( w_{k+1} = w | w_1, w_2, ..., w_{k})$\n",
    "\n",
    "Οι λόγοι που θέλουμε μια τέτοια μοντελοποίηση στην πράξη:\n",
    "\n",
    "- spell checking \n",
    "- αναγνώριση φωνής \n",
    "- autocomplete \n",
    "- ... \n",
    "\n",
    "Συνήθως, τέτοια μόντέλα υπολογίζονται σε (τεράστιες) συλλογές κειμένων. Στη φυσική γλώσσα όμως, είναι βέβαιο ότι στην πράξη θα εμφανιστεί μια ακολουθία λέξεων που δεν υπάρχει στη συλλογή μας και θέλουμε να αποφύγουμε το μοντέλο μας να δώσει μηδενική πιθανότητα σε μια ακολουθία λέξεων. Παραδοσιακά, τέτοια προβλήματα λύνονταν με κάποιας μορφής παρεμβολή/προσέγγιση.\n",
    "\n",
    "\n",
    "#### Neural models \n",
    "\n",
    "Το 2003, ο Bengio πρότεινε η συνάρτηση $p(w)$ να υπολογίζεται με εκπαίδευση νευρωνικού δικτύου στη συλλογή κειμένων,\n",
    "έτσι ώστε η εισαγωγή στο δίκτυο να είναι k διανύσματα συνεχόμενων λέξεων ($w_1,...,w_k$) και το δίκτυο να προσπαθεί να προβλέψει ως έξοδο τη λέξη $w_{k+1}$. Η τεχνική αυτή είχε πολύ καλά αποτελέσματα αλλά ήταν πολύ αργή στην εκπαίδευση του νευρωνικού. \n",
    "\n",
    "\n",
    "Fast forward, 2013, ο Mikolov προτείνει το ίδιο ουσιαστικά μοντέλο αλλά με κάποιες σημαντικές διαφοροποιήσεις/ευρεστικές μεθόδους για την επιτάχυνση της εκπαίδευσης αλλά και την αρχιτεκτονική. το μοντέλο αυτό ονομάστηκε word2vec και ήταν η αρχή μιας τεράστιας έκρηξης στην επεξεργασία φυσικής γλώσσας.\n",
    "\n",
    "\n",
    "Στην πράξη, αν πάρουμε για παράδειγμα τα κείμενα του imbd, που μπορεί να είναι αρκετές δεκάδες χιλιάδες λέξεων, το word2vec μας δίνει πίσω μια $D$-διάστατη απεικόνηση κάθε μιας από τις λέξεις, με $D << |V|$\n",
    "\n",
    "\n",
    "\n",
    "Αρχιτεκτονικές CBOW και Skip-Gram\n",
    "\n",
    "![αρχιτεκτονικές](O2YeO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Παράδειγμα word2vec με τη βιβλιοθήκη gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyCorpus(object):\n",
    "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "    \n",
    "    def __init__(self, files):\n",
    "        self.files = files\n",
    "            \n",
    "    def __iter__(self):\n",
    "        for file in self.files:\n",
    "            \n",
    "            text = open( file ).read().lower()\n",
    "            \n",
    "            yield simple_preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are', 'lot', 'of', 'memorable', 'quotes', 'listed', 'for', 'this', 'gem', 'imagine', 'movie', 'where', 'joe', 'piscopo', 'is', 'actually', 'funny', 'maureen', 'stapleton', 'is', 'scene', 'stealer', 'the', 'moroni', 'character', 'is', 'an', 'absolute', 'scream', 'watch', 'for', 'alan', 'the', 'skipper', 'hale', 'jr', 'as', 'police', 'sgt']\n"
     ]
    }
   ],
   "source": [
    "sentences = MyCorpus(positive_train + negative_train)\n",
    "for s in sentences:\n",
    "    print(s)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MyCorpus(positive_train + negative_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec( min_count=5, workers=5, size=200) \n",
    "model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21181404, 28265970)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = MyCorpus(positive_train + negative_train)\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.8418352603912354),\n",
       " ('horrible', 0.8121166825294495),\n",
       " ('amazing', 0.7599561214447021),\n",
       " ('atrocious', 0.7265543341636658),\n",
       " ('dreadful', 0.7250735759735107),\n",
       " ('awesome', 0.7141908407211304),\n",
       " ('laughable', 0.6928317546844482),\n",
       " ('ridiculous', 0.6859292387962341),\n",
       " ('bad', 0.6758812665939331),\n",
       " ('abysmal', 0.6707140803337097)]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"awful\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.84183526, ('terrible', 0.8418352603912354))"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "v1 = model.wv[\"awful\"].reshape(1,-1)\n",
    "v2 = model.wv[\"terrible\"].reshape(1,-1)\n",
    "\n",
    "\n",
    "cosine_similarity( v1, v2)[0][0], model.wv.most_similar(\"awful\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrific', 0.9072020649909973),\n",
       " ('fantastic', 0.8522029519081116),\n",
       " ('magnificent', 0.8338733315467834),\n",
       " ('flawless', 0.8325532674789429),\n",
       " ('outstanding', 0.8291945457458496),\n",
       " ('brilliant', 0.827364981174469),\n",
       " ('marvelous', 0.823667049407959),\n",
       " ('splendid', 0.8175074458122253),\n",
       " ('excellent', 0.8133594989776611),\n",
       " ('exceptional', 0.8094056844711304)]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"superb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3489405 , -0.4663679 ,  0.07514985, -0.11252509,  0.63954186,\n",
       "        0.6424196 ,  0.31715065,  0.42646608, -0.3214422 , -0.82062745,\n",
       "       -1.3028479 ,  0.6060892 ,  1.0367268 ,  0.19444737,  0.00637792,\n",
       "       -0.3790396 , -0.11567144, -0.52008176, -0.56881994, -0.5277116 ,\n",
       "        1.2998116 , -0.26179138, -0.35718074,  0.4069805 , -1.1577462 ,\n",
       "        0.7011286 , -0.02364672, -0.40308866, -0.40020722, -1.6058701 ,\n",
       "        0.1491558 ,  0.01351472, -1.8364967 ,  0.19110866, -0.5087051 ,\n",
       "       -0.2547878 ,  0.2595665 , -0.28101715, -0.6030559 , -1.2741352 ,\n",
       "        0.35360038,  0.06381977,  0.89256   , -1.0443642 ,  0.4820704 ,\n",
       "       -0.523485  ,  0.07598894,  0.43283203, -0.05821151,  0.00934027,\n",
       "       -0.26078805,  0.05516728,  0.07939304, -0.37620443,  0.52051574,\n",
       "        0.7022746 , -0.16445833, -0.6009418 , -0.6301671 ,  0.7340323 ,\n",
       "        0.4210765 ,  0.18928401, -0.63667876, -0.25318676, -0.21367818,\n",
       "       -0.74385947, -0.13740134,  0.31397378,  0.6059355 , -0.5225892 ,\n",
       "        0.17949572, -0.14884917, -0.2837944 , -0.30599433,  0.5001254 ,\n",
       "        0.34844297,  0.15788624,  0.413673  , -1.2564551 ,  0.13698077,\n",
       "       -0.46534047,  0.28505698, -1.2856289 ,  0.47978866, -0.99030155,\n",
       "       -0.6242808 , -0.5824537 , -0.44190526,  1.2439874 , -0.6414936 ,\n",
       "        0.35128433,  0.6708595 ,  1.0263256 ,  1.3412915 , -0.4001018 ,\n",
       "        0.19376697,  0.24420847, -1.2796104 , -0.1950466 , -0.22679211,\n",
       "       -1.1426122 , -0.40162474, -0.42572993, -0.395077  ,  0.2696118 ,\n",
       "        1.1741776 , -0.40350294,  0.8456908 ,  0.6325911 , -0.62687725,\n",
       "       -0.3929116 ,  0.856385  ,  2.4544444 ,  0.8675097 ,  0.29730454,\n",
       "        0.5709015 ,  0.00709433,  0.29682314, -1.1962018 ,  0.9634903 ,\n",
       "        0.23179802, -0.46066642, -1.4685291 , -0.5076155 ,  0.40251422,\n",
       "       -0.7736208 ,  0.2925618 , -0.5330868 ,  0.97321993,  0.31562814,\n",
       "        0.5717538 ,  0.24202207,  1.5973718 , -1.3143219 ,  0.36771837,\n",
       "       -1.1915827 ,  0.5089457 , -0.86600786,  1.1726215 , -0.49714926,\n",
       "       -0.5650441 ,  0.5686939 ,  1.0710647 , -0.12006469,  0.29388732,\n",
       "        0.88800013,  0.33813024,  0.7213165 , -0.6456745 ,  0.7340841 ,\n",
       "        0.01346707, -0.42746988, -0.16918504, -0.64899176, -0.78247094,\n",
       "        0.13215764, -1.7233356 ,  0.00976525, -0.46851075, -0.06870584,\n",
       "       -1.3547735 ,  1.219162  , -0.88793397,  0.5202979 ,  0.0438824 ,\n",
       "        0.62472165, -0.9444763 ,  0.03015338, -0.43908462,  0.02247373,\n",
       "        0.4279111 , -0.9633888 ,  1.2609724 , -0.5271419 , -0.99965185,\n",
       "        0.0648642 , -1.2523892 , -1.2878158 , -0.65101844,  0.5096533 ,\n",
       "        0.44427392, -0.2409075 , -0.5143805 , -0.77126265,  0.53855324,\n",
       "        0.30486688, -0.71214616,  0.6845417 ,  0.07306258,  1.1062907 ,\n",
       "       -0.4261724 ,  0.2647154 , -0.25791386, -1.2231886 ,  0.25023076,\n",
       "        0.30683118, -0.7032599 , -0.696665  , -0.25302288, -0.02246107],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"superb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./data/model_v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Word2Vec()\n",
    "model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"./data/model_v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Εφαρμογή: IMDB reviews sentiment classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "D=200\n",
    "nwords = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Απλή προσέγγιση: Η αναπαράσταση του κάθε κειμένου είναι ο μέσος όρος των διανυσμάτων των λέξεων του κειμένου"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_vector( text, model, D,  nwords=1000 ):\n",
    "    words = simple_preprocess(open(text).read())[0:nwords]\n",
    "    \n",
    "    c = 0 \n",
    "    v = np.zeros(D)\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            c +=1 \n",
    "            v+= model.wv[word]\n",
    "        \n",
    "      \n",
    "    return v/c\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos = np.zeros( (len(positive_train), D))\n",
    "y_pos = np.ones( len(positive_train) )\n",
    "\n",
    "for idx,f in enumerate(positive_train):\n",
    "    X_pos[idx,:] = file_to_vector(f, model, D=D, nwords=nwords )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12500, 200), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pos.shape, y_pos[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_neg = np.zeros( (len(negative_train), D))\n",
    "y_neg = np.zeros( len(negative_train) )\n",
    "\n",
    "for idx,f in enumerate(negative_train):\n",
    "    X_neg[idx,:] = file_to_vector(f, model, D=D, nwords=nwords )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12500, 200), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_neg.shape, y_neg[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate( (X_pos, X_neg) , axis=0)\n",
    "y= np.concatenate(  (y_pos, y_neg) , axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessing.scale(X, axis=0) # zero mean, unit variance for each vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18750, 200), (6250, 200))"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train , y_test = train_test_split( X, y, random_state =42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Λογιστική παλινδρόμιση για την ταξινόμιση των κειμένων: $p(l=1|text) = \\frac{1}{1+exp(-x^T \\cdot w)}$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1500,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter = 1500, random_state = 42,fit_intercept=True) \n",
    "clf.fit( X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6250, 200)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8216"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = clf.predict(X_test)\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 82% ακρίβεια πρόβλεψης με ένα απλό παράδειγμα "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98037745]])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = model.wv[\"fantastic\"]\n",
    "v1 = np.append( v1, 1)\n",
    "w =  np.append( clf.coef_, clf.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9796366717675539,)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = model.wv[\"fantastic\"]\n",
    "v1 = np.append( v1, 1)\n",
    "w =  np.append( clf.coef_, clf.intercept_)\n",
    "\n",
    "1/(1+np.exp(-v1.dot(w))), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02036333, 0.97963667]])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba( [model.wv[\"fantastic\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba( [model.wv[\"fantastic\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [refs]\n",
    "- Neural language model, Bengio : http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "- word2vec, Mikolov: https://arxiv.org/abs/1301.3781\n",
    "- gensim: https://radimrehurek.com/gensim/ \n",
    "- Neural embeddings for metaphor detection: https://arxiv.org/pdf/1902.03659.pdf \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
